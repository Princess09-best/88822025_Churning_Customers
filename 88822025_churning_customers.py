# -*- coding: utf-8 -*-
"""88822025_Churning_Customers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dbUajiBJPca-AzuUCH-F3bvnDnrRCKHu

***CHURNING CUSTOMERS IN A TELECOMS COMPANY***

**IMPORTATION OF NECCESSARY LIBRARIES AND MOUNTING OF** **DRIVE**
"""

# Data manipulation and analysis
import pandas as pd
import numpy as np



# Machine learning libraries
import sklearn as sk
import tensorflow as tf

from tensorflow.keras import layers,models
from tensorflow.keras.layers import Input,Dense
from tensorflow.keras.models import Model


from sklearn.model_selection import GridSearchCV, StratifiedKFold,RandomizedSearchCV

from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.model_selection import cross_val_score

from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler

from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectFromModel


# Data Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Additional Libraries
from google.colab import drive

!pip install scikeras

from scikeras.wrappers import KerasClassifier

# Mounts the Google Drive to the specified directory '/content/drive'
# This allows access to files and data stored in your Google Drive within the Colab environment
drive.mount('/content/drive')

"""***DATA COLLECTION***"""

# Reasa CSV file into a Pandas DataFrame
# The file 'player_21.csv' is located in the specified Google Drive directory and is loaded into the variabe 'first_dataset' for further data analysis
dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CustomerChurn_dataset.csv')

dataset

dataset.info()

dataset['TotalCharges']=pd.to_numeric(dataset['TotalCharges'],errors='coerce')

dataset.TotalCharges.isna().sum()

dataset.dropna(subset=['TotalCharges'],inplace=True)

dataset.isna().sum()

dataset.reset_index(drop=True)

dataset.info()

"""**EXPLORATORY DATA ANALYSIS AND DATA PREPROCESSING**"""

# Access and display the column labels (column names) of Pandas DataFrame
dataset.columns

# Select columns with numeric data types
num1=dataset.select_dtypes(include=['number'])

# Select columns with non-numeric
obj1=dataset.select_dtypes(exclude=['number'])

num1.isnull().sum()

# This includes statistics like count, mean, standard deviation, minimum, and quartiles for each numeric columns
num1.describe()

obj1.info()

obj1.isnull().any()

print(obj1.columns)

# Import necessary preprocessing tools from scikit-learn
from sklearn.preprocessing import LabelEncoder,StandardScaler

#List of column names to be encoded
columns=['customerID', 'gender', 'Partner', 'Dependents', 'PhoneService',
       'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',
       'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',
       'Contract', 'PaperlessBilling', 'PaymentMethod',
       'Churn']

# Create a dictonary to store LabelEncoder instances for each colmun
dict_obj = {}

# Iterate through the list of columns and apply LabelEncoder to each
for col in columns:
  obj=LabelEncoder()
  obj1[col]=obj.fit_transform(obj1[col])
dict_obj[col]=obj

obj1

obj1.isnull().sum()

# The resulting DataFrame contains both encoded categorical and imputed numeric data
new_dataset= pd.concat([obj1,num1],axis=1)

print(new_dataset.columns)

"""***Feature Selection and Engineering***"""

#exploring possible corelatons of features with churn
#Extract target variable
target=new_dataset['Churn']
features=new_dataset.drop('Churn',axis=1)

corr_with_target = features.corrwith(target)
corr_with_target= corr_with_target.sort_values(ascending=False)

#visualising on heat map
plt.figure(figsize=(4,8))
sns.heatmap(corr_with_target.to_frame(),cmap='GnBu',annot=True)
plt.title('Feature correlation with Churn')
plt.show()

RandomForestClassifier,
#Feature importance to select features
rf=RandomForestClassifier()
rf.fit(features,target)
importances= rf.feature_importances_

feature_importance_df=pd.DataFrame({'Features':features.columns,'Score':importances})
feature_importance_df=feature_importance_df.sort_values(by='Score',ascending=False)
print(feature_importance_df)

#Visualising feature importances in relation to churn
plt.figure(figsize=(12,6))
plt.bar(range(features.shape[1]),importances)
plt.xticks(range(features.shape[1]),features.columns,rotation=90)
plt.ylabel=('Feature importance scores')
plt.xlabel('features')
plt.title('Feature importance for Customer churn')
plt.show()

#selecting the first 5 features and the target variable
best_features_with_target= ['customerID','TotalCharges','MonthlyCharges','tenure','Contract','Churn']
chosen_features=new_dataset[best_features_with_target]

chosen_features

"""Analysis of Customer profile relation to Churn

"""

#from dataset exploration, we can see the customer profiles are Senior Citizen,Gender,Partner,and Dependents
#we will examine churn in relation to these profiles

#Function to analyse yes/no variables

def churn_variance_distribution(col_name):
  churn_negative = ((dataset[dataset['Churn']=='No'][col_name].value_counts()))/(
  dataset[dataset['Churn']=='No'][col_name].value_counts().sum())

  churn_positive = ((dataset[dataset['Churn']=='Yes'][col_name].value_counts()))/(
  dataset[dataset['Churn']=='Yes'][col_name].value_counts().sum())


  x_labels = dataset['Churn'].value_counts().keys().tolist()
  negative_variance = [churn_negative['No'],churn_positive['No']]
  positive_variance = [churn_negative['Yes'],churn_positive['Yes']]

  barWidth = 0.8
  plt.figure(figsize=(7,7))
  ax1 = plt.bar(x_labels, negative_variance, color='#00BFFF',
                  label=('No ' + col_name), edgecolor='white', width=barWidth)
  ax2 = plt.bar(x_labels, positive_variance, bottom=negative_variance, color='lightgreen',
                  label=('Yes ' + col_name), edgecolor='white', width=barWidth)
  plt.legend()
  plt.title('Churn Distribution by ' + col_name)

  for r1, r2 in zip(ax1, ax2):
      h1=r1.get_height()
      h2 = r2.get_height()
      plt.text(r1.get_x() + r1.get_width() / 2., h1 / 2., '{:.2%}'.format(h1),
              ha='center', va='center', color='black', fontweight='bold')
      plt.text(r2.get_x() + r2.get_width() / 2., h1 + h2 / 2., '{:.2%}'.format(h2),
              ha='center', va='center', color='black', fontweight='bold')

  plt.show()

churn_variance_distribution('Partner')

"""From this we can see that customers without partners are more related to the churn and churn more than customers with partners"""

churn_variance_distribution('Dependents')

"""This means that customers without dependents churn a lot more, with a percentage of 82.56%"""

churn_numeric = {'Yes':1, 'No':0}
dataset.Churn.replace(churn_numeric, inplace=True)

#analysis for categorical values where category is not yes or no

dataset[['SeniorCitizen','Churn']].groupby(['SeniorCitizen']).mean()

"""Senior Citizens churn a lot more that non-Senior Citizens"""

dataset[['gender','Churn']].groupby(['gender']).mean()

"""2. Answer: This means that Senior citizen females who have no partners and no dependents have the most churn

Females churn more than males, although the difference is very slight

*Data Scaling and Splitting*
"""

# Remove the 'overall' column from the DataFrame 'features'to create the feature matrix 'X'
X = chosen_features.drop("Churn",axis=1)

# Extract the 'overall'colum as the target variable 'y'
y = chosen_features["Churn"]

# Create a StandardScaler instance to standardize the feature matrx 'X'
scaler = StandardScaler()

# Standardize the feature matrix 'X'
X_scaled = scaler.fit_transform(X)
# X_test_scaled = scaler.transform(X_test)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

X_train

X_test



"""***Building and   Training Multilayer Perceptron Using Functional API***"""

def model_creation(optimizer='adam', activation='relu'):
    input_layer = Input(shape=(X_train.shape[1],))
    hiddenlayer1 = Dense(128, activation='relu')(input_layer)
    hiddenlayer2 = Dense(64, activation='tanh')(hiddenlayer1)
    hiddenlayer3 = Dense(32, activation='elu')(hiddenlayer2)
    hiddenlayer4 = Dense(8, activation='selu')(hiddenlayer3)
    hiddenlayer5 = Dense(8, activation='softmax')(hiddenlayer4)
    output_layer = Dense(1, activation='sigmoid')(hiddenlayer5)

    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
    return model

model = KerasClassifier(build_fn=model_creation, epochs=10, activation='relu',batch_size=32, verbose=0)

param_grid = {'batch_size': [10, 15,20],
         'epochs': [10,15],
         'optimizer': ['adam', 'rmsprop'],
         'loss': ['binary_crossentropy', 'hinge'],

         'activation': ['relu', 'tanh','softmax','sigmoid'],
         }

kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

# Define hyperparameter distributions for RandomizedSearchCV
param_dist = {
    'optimizer': ['adam', 'rmsprop'],
    'activation': ['relu', 'tanh', 'sigmoid']
}

# Create RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, scoring='roc_auc', cv=kfold, n_iter=10)
random_result = random_search.fit(X_train, y_train)

# Get the best model from RandomizedSearchCV
best_random_model = random_result.best_estimator_

"""Training And retesting optimised Model"""

y_pred_random = best_random_model.predict(X_test)
random_accuracy = accuracy_score(y_test, y_pred_random)
random_auc = roc_auc_score(y_test, y_pred_random)

print(f'Randomized Search Accuracy: {random_accuracy}')
print(f'Randomized Search AUC: {random_auc}')

"""Saving the model"""

import pickle
finalfile=open('trained_model.pkl','wb')
pickle.dump(best_random_model,finalfile)